{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# dataset used: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "# Build datasets for training and testing\n",
    "#####################################################################################\n",
    "\n",
    "datapath = '../datasets/sentiment'\n",
    "file_names = ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n",
    "\n",
    "\n",
    "def get_lexicon():\n",
    "    lexicon = []\n",
    "    lex2 = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for file_name in file_names:\n",
    "        with open(os.path.join(datapath, file_name), 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                words = word_tokenize(line.split('\\t')[0])\n",
    "                lexicon += list(words)\n",
    "    lexicon = [lemmatizer.lemmatize(_.lower()) for _ in lexicon if len(_)>2]\n",
    "    \n",
    "    word_counts = Counter(lexicon)\n",
    "    for w in word_counts:\n",
    "        if  500 > word_counts[w] > 10:\n",
    "            lex2.append(w)\n",
    "    print(len(lex2))\n",
    "    return lex2\n",
    "\n",
    "\n",
    "def get_features(lexicon):\n",
    "    featureset = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        with open(os.path.join(datapath, file_name), 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                features = np.zeros(len(lexicon))\n",
    "                line, pol = line.split('\\t')\n",
    "                pol = int(pol)\n",
    "                if pol == 0:\n",
    "                    pol = [0,1]\n",
    "                else:\n",
    "                    pol = [1,0]\n",
    "                words = word_tokenize(line)\n",
    "                for word in words:\n",
    "                    word = word.lower()\n",
    "                    if word in lexicon:\n",
    "                        idx = lexicon.index(word)\n",
    "                        features[idx] += 1\n",
    "                features = list(features)\n",
    "                        \n",
    "                featureset.append([features, pol])\n",
    "    return featureset\n",
    "   \n",
    "\n",
    "def create_train_test_features(test_size=0.2):\n",
    "    \n",
    "    lexicon = get_lexicon()\n",
    "    features = np.array(get_features(lexicon))\n",
    "    \n",
    "    size = len(features)\n",
    "    train_size = int(size - size*test_size)\n",
    "    \n",
    "    train_x = list(features[:,0][:train_size])\n",
    "    train_y = list(features[:,1][:train_size])\n",
    "    test_x = list(features[:,0][train_size:])\n",
    "    test_y = list(features[:,1][train_size:])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "# train_x, train_y, test_x, test_y = create_train_test_features()\n",
    "# with open(os.path.join(datapath, 'sentiment_set.pkl'), 'wb') as file:\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-43-bfdbb7d97a74>:56: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch:  0  Completed out of:  10  loss:  397286.949707\n",
      "Epoch:  1  Completed out of:  10  loss:  165049.135254\n",
      "Epoch:  2  Completed out of:  10  loss:  165852.724243\n",
      "Epoch:  3  Completed out of:  10  loss:  130939.307251\n",
      "Epoch:  4  Completed out of:  10  loss:  30148.7880554\n",
      "Epoch:  5  Completed out of:  10  loss:  12226.3799133\n",
      "Epoch:  6  Completed out of:  10  loss:  6978.53742218\n",
      "Epoch:  7  Completed out of:  10  loss:  4871.41127396\n",
      "Epoch:  8  Completed out of:  10  loss:  2697.82446218\n",
      "Epoch:  9  Completed out of:  10  loss:  3075.82834148\n",
      "Accuracy: 0.703333\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# Build Classification model using Tensorflow\n",
    "#####################################################################################\n",
    "\n",
    "pkl_file_name = 'sentiment_set.pkl'\n",
    "# pkl_file_name = 'sentex.pkl'\n",
    "\n",
    "train_x, train_y, test_x, test_y  = [],[],[],[]\n",
    "with open(os.path.join(datapath, pkl_file_name), 'rb') as file:\n",
    "    train_x, train_y, test_x, test_y = pickle.load(file)\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "nn_nodes_hl1 = 1500\n",
    "nn_nodes_hl2 = 1500\n",
    "nn_nodes_hl3 = 1500\n",
    "\n",
    "x = tf.placeholder('float', [None, len(train_x[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_layer1 = {'weights': tf.Variable(tf.random_normal([len(train_x[0]), nn_nodes_hl1])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_nodes_hl1]))}\n",
    "    \n",
    "    hidden_layer2 = {'weights': tf.Variable(tf.random_normal([nn_nodes_hl1, nn_nodes_hl2])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_nodes_hl2]))}\n",
    "    \n",
    "    hidden_layer3 = {'weights': tf.Variable(tf.random_normal([nn_nodes_hl2, nn_nodes_hl3])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([nn_nodes_hl3, n_classes])),\n",
    "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_layer1['weights']), hidden_layer1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer2['weights']), hidden_layer2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer3['weights']), hidden_layer3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                \n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                \n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "                \n",
    "            print('Epoch: ', epoch, ' Completed out of: ', num_epochs, ' loss: ', epoch_loss)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
