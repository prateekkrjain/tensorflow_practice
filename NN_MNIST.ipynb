{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from struct import unpack\n",
    "from numpy import zeros, uint8, float32, ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# dataset used: http://yann.lecun.com/exdb/mnist/\n",
    "# Prepare data for training and testing\n",
    "#####################################################################################\n",
    "\n",
    "datapath = '../datasets/mnist'\n",
    "n_classes = 10\n",
    "\n",
    "def get_labeled_data(imagefile, labelfile, picklename):\n",
    "    imagefile = os.path.join(datapath, imagefile)\n",
    "    labelfile = os.path.join(datapath, labelfile)\n",
    "    picklename = os.path.join(datapath, picklename)\n",
    "    \n",
    "    if os.path.isfile('%s.pickle' % picklename):\n",
    "        data = pickle.load(open('%s.pickle' % picklename, 'rb'))\n",
    "    else:\n",
    "        # Open the images with gzip in read binary mode\n",
    "        images = gzip.open(imagefile, 'rb')\n",
    "        labels = gzip.open(labelfile, 'rb')\n",
    "\n",
    "        # Read the binary data\n",
    "\n",
    "        # We have to get big endian unsigned int. So we need '>I'\n",
    "\n",
    "        # Get metadata for images\n",
    "        images.read(4)  # skip the magic_number\n",
    "        number_of_images = images.read(4)\n",
    "        number_of_images = unpack('>I', number_of_images)[0]\n",
    "        rows = images.read(4)\n",
    "        rows = unpack('>I', rows)[0]\n",
    "        cols = images.read(4)\n",
    "        cols = unpack('>I', cols)[0]\n",
    "\n",
    "        # Get metadata for labels\n",
    "        labels.read(4)  # skip the magic_number\n",
    "        N = labels.read(4)\n",
    "        N = unpack('>I', N)[0]\n",
    "\n",
    "        if number_of_images != N:\n",
    "            raise Exception('The number of labels did not match '\n",
    "                            'the number of images.')\n",
    "\n",
    "        # Get the data\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(N):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: %i\" % i)\n",
    "            temp_x = []\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    tmp_pixel = images.read(1)  # Just a single byte\n",
    "                    tmp_pixel = unpack('>B', tmp_pixel)[0]\n",
    "                    temp_x.append(float(tmp_pixel) / 255)\n",
    "            x.append(temp_x)\n",
    "            tmp_label = labels.read(1)\n",
    "            y.append(unpack('>B', tmp_label)[0])\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        data = [x,y]\n",
    "        pickle.dump(data, open(\"%s.pickle\" % picklename, \"wb\"))\n",
    "    return data\n",
    "\n",
    "train_x, train_y = get_labeled_data('train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz', 'train')\n",
    "test_x, test_y = get_labeled_data('t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz', 'test')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_y = sess.run(tf.one_hot(indices=train_y, depth=n_classes, on_value=1., off_value=0.))\n",
    "    test_y = sess.run(tf.one_hot(indices=test_y, depth=n_classes, on_value=1., off_value=0.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Out of:  10  loss:  5418978.34674\n",
      "Epoch:  1  Out of:  10  loss:  1216854.67543\n",
      "Epoch:  2  Out of:  10  loss:  560792.197189\n",
      "Epoch:  3  Out of:  10  loss:  327129.762535\n",
      "Epoch:  4  Out of:  10  loss:  231620.476891\n",
      "Epoch:  5  Out of:  10  loss:  187364.331367\n",
      "Epoch:  6  Out of:  10  loss:  159485.793633\n",
      "Epoch:  7  Out of:  10  loss:  134365.779572\n",
      "Epoch:  8  Out of:  10  loss:  110811.053268\n",
      "Epoch:  9  Out of:  10  loss:  120428.96904\n",
      "Accuracy:  0.9598\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.001\n",
    "n_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "nn_node_hl1 = 1500\n",
    "nn_node_hl2 = 1500\n",
    "nn_node_hl3 = 1500\n",
    "num_features = len(train_x[0])\n",
    "\n",
    "x = tf.placeholder('float', [None, num_features])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "\n",
    "def create_nueral_network(data):\n",
    "    hidden_layer1 = {'weights': tf.Variable(tf.random_normal([num_features, nn_node_hl1])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_node_hl1]))}\n",
    "    \n",
    "    hidden_layer2 = {'weights': tf.Variable(tf.random_normal([nn_node_hl1, nn_node_hl2])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_node_hl2]))}\n",
    "    \n",
    "    hidden_layer3 = {'weights': tf.Variable(tf.random_normal([nn_node_hl2, nn_node_hl3])),\n",
    "                    'biases': tf.Variable(tf.random_normal([nn_node_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([nn_node_hl3, n_classes])),\n",
    "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_layer1['weights']), hidden_layer1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer2['weights']), hidden_layer2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer3['weights']), hidden_layer3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = create_nueral_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                \n",
    "                batch_x = train_x[start:end]\n",
    "                batch_y = train_y[start:end]\n",
    "                \n",
    "                _, c = sess.run([optimize, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                \n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "            print('Epoch: ', epoch, ' Out of: ', num_epochs, ' loss: ', epoch_loss)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "        print('Accuracy: ', accuracy.eval({x: test_x, y: test_y}))\n",
    "        \n",
    "\n",
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
